{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop - Regularization\n",
    "\n",
    "In this workshop, we are going to:\n",
    "\n",
    "1. Tune an elastic-net regression \n",
    "2. Compare the following models:\n",
    "    1. The null model\n",
    "    2. The tuned elastic-net model\n",
    "    3. The trimmed non-regularized model with standardized features\n",
    "    4. The trimmed non-regularized model with non-standardized features\n",
    "    \n",
    "# Preliminaries\n",
    "\n",
    "- Load any necessary packages and/or functions\n",
    "- Load in and prepare the class data\n",
    "- Create x and y with a label of `pct_d_rgdp`\n",
    "- Create `x_train`, `x_test`, `y_train`, `y_test` with\n",
    "    * training size of two-thirds\n",
    "    * random state of 490\n",
    "- Standardize the features\n",
    "- Add constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn import linear_model as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:/Users/Chris/Documents/Spring 2021/Module 2 Stuff/class_data.pkl')\n",
    "df.columns\n",
    "\n",
    "df_prepped = df.drop(columns = ['urate_bin', 'year']).join([\n",
    "    pd.get_dummies(df['urate_bin'], drop_first = True),\n",
    "    pd.get_dummies(df.year, drop_first = True)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_prepped['pct_d_rgdp']\n",
    "x = df_prepped.drop(columns = 'pct_d_rgdp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 2/3, random_state = 490)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_std = x_train.apply(lambda x: (x - np.mean(x))/np.std(x), axis = 0)\n",
    "x_test_std  = x_test.apply(lambda x: (x - np.mean(x))/np.std(x), axis = 0)\n",
    "\n",
    "x_train_std = sm.add_constant(x_train_std)\n",
    "x_test_std  = sm.add_constant(x_test_std)\n",
    "x_train     = sm.add_constant(x_train)\n",
    "x_test      = sm.add_constant(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at `lm.ElasticNet?` and \n",
    "```\n",
    "fit = sm.OLS(y_train, x_train)\n",
    "fit.fit_regularized?\n",
    "```\n",
    "Determine which coefficients are the same, but named differently.\n",
    "Specifically, $\\alpha$ and the weight on the different constraints (i.e. $||\\beta||_2$ and $||\\beta||_1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mElasticNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprecompute\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mselection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cyclic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear regression with combined L1 and L2 priors as regularizer.\n",
       "\n",
       "Minimizes the objective function::\n",
       "\n",
       "        1 / (2 * n_samples) * ||y - Xw||^2_2\n",
       "        + alpha * l1_ratio * ||w||_1\n",
       "        + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
       "\n",
       "If you are interested in controlling the L1 and L2 penalty\n",
       "separately, keep in mind that this is equivalent to::\n",
       "\n",
       "        a * L1 + b * L2\n",
       "\n",
       "where::\n",
       "\n",
       "        alpha = a + b and l1_ratio = a / (a + b)\n",
       "\n",
       "The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
       "alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
       "= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
       "unless you supply your own sequence of alpha.\n",
       "\n",
       "Read more in the :ref:`User Guide <elastic_net>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, default=1.0\n",
       "    Constant that multiplies the penalty terms. Defaults to 1.0.\n",
       "    See the notes for the exact mathematical meaning of this\n",
       "    parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n",
       "    solved by the :class:`LinearRegression` object. For numerical\n",
       "    reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
       "    Given this, you should use the :class:`LinearRegression` object.\n",
       "\n",
       "l1_ratio : float, default=0.5\n",
       "    The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
       "    ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
       "    is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
       "    combination of L1 and L2.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether the intercept should be estimated or not. If ``False``, the\n",
       "    data is assumed to be already centered.\n",
       "\n",
       "normalize : bool, default=False\n",
       "    This parameter is ignored when ``fit_intercept`` is set to False.\n",
       "    If True, the regressors X will be normalized before regression by\n",
       "    subtracting the mean and dividing by the l2-norm.\n",
       "    If you wish to standardize, please use\n",
       "    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
       "    on an estimator with ``normalize=False``.\n",
       "\n",
       "precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
       "    Whether to use a precomputed Gram matrix to speed up\n",
       "    calculations. The Gram matrix can also be passed as argument.\n",
       "    For sparse input this option is always ``True`` to preserve sparsity.\n",
       "\n",
       "max_iter : int, default=1000\n",
       "    The maximum number of iterations\n",
       "\n",
       "copy_X : bool, default=True\n",
       "    If ``True``, X will be copied; else, it may be overwritten.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    The tolerance for the optimization: if the updates are\n",
       "    smaller than ``tol``, the optimization code checks the\n",
       "    dual gap for optimality and continues until it is smaller\n",
       "    than ``tol``.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "positive : bool, default=False\n",
       "    When set to ``True``, forces the coefficients to be positive.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    The seed of the pseudo random number generator that selects a random\n",
       "    feature to update. Used when ``selection`` == 'random'.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "selection : {'cyclic', 'random'}, default='cyclic'\n",
       "    If set to 'random', a random coefficient is updated every iteration\n",
       "    rather than looping over features sequentially by default. This\n",
       "    (setting to 'random') often leads to significantly faster convergence\n",
       "    especially when tol is higher than 1e-4.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
       "    parameter vector (w in the cost function formula)\n",
       "\n",
       "sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
       "    ``sparse_coef_`` is a readonly property derived from ``coef_``\n",
       "\n",
       "intercept_ : float or ndarray of shape (n_targets,)\n",
       "    independent term in decision function.\n",
       "\n",
       "n_iter_ : list of int\n",
       "    number of iterations run by the coordinate descent solver to reach\n",
       "    the specified tolerance.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.linear_model import ElasticNet\n",
       ">>> from sklearn.datasets import make_regression\n",
       "\n",
       ">>> X, y = make_regression(n_features=2, random_state=0)\n",
       ">>> regr = ElasticNet(random_state=0)\n",
       ">>> regr.fit(X, y)\n",
       "ElasticNet(random_state=0)\n",
       ">>> print(regr.coef_)\n",
       "[18.83816048 64.55968825]\n",
       ">>> print(regr.intercept_)\n",
       "1.451...\n",
       ">>> print(regr.predict([[0, 0]]))\n",
       "[1.451...]\n",
       "\n",
       "\n",
       "Notes\n",
       "-----\n",
       "To avoid unnecessary memory duplication the X argument of the fit method\n",
       "should be directly passed as a Fortran-contiguous numpy array.\n",
       "\n",
       "See also\n",
       "--------\n",
       "ElasticNetCV : Elastic net model with best model selection by\n",
       "    cross-validation.\n",
       "SGDRegressor: implements elastic net regression with incremental training.\n",
       "SGDClassifier: implements logistic regression with elastic net penalty\n",
       "    (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\chris\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     Lasso\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.ElasticNet?\n",
    "#lm_fit = lm.LinearRegression().fit()\n",
    "\n",
    "#alpha = 1, l1_ratio =  .5\n",
    "\n",
    "#Unsure how to enter in the arguements :/\n",
    "#lm_elastic = lm_fit.ElasticNet(alpha = 1, x_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_regularized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'elastic_net'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mL1_wt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstart_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprofile_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Return a regularized fit to a linear regression model.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "method : str\n",
       "    Either 'elastic_net' or 'sqrt_lasso'.\n",
       "alpha : scalar or array_like\n",
       "    The penalty weight.  If a scalar, the same penalty weight\n",
       "    applies to all variables in the model.  If a vector, it\n",
       "    must have the same length as `params`, and contains a\n",
       "    penalty weight for each coefficient.\n",
       "L1_wt : scalar\n",
       "    The fraction of the penalty given to the L1 penalty term.\n",
       "    Must be between 0 and 1 (inclusive).  If 0, the fit is a\n",
       "    ridge fit, if 1 it is a lasso fit.\n",
       "start_params : array_like\n",
       "    Starting values for ``params``.\n",
       "profile_scale : bool\n",
       "    If True the penalized fit is computed using the profile\n",
       "    (concentrated) log-likelihood for the Gaussian model.\n",
       "    Otherwise the fit uses the residual sum of squares.\n",
       "refit : bool\n",
       "    If True, the model is refit using only the variables that\n",
       "    have non-zero coefficients in the regularized fit.  The\n",
       "    refitted model is not regularized.\n",
       "**kwargs\n",
       "    Additional keyword arguments that contain information used when\n",
       "    constructing a model using the formula interface.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "statsmodels.base.elastic_net.RegularizedResults\n",
       "    The regularized results.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The elastic net uses a combination of L1 and L2 penalties.\n",
       "The implementation closely follows the glmnet package in R.\n",
       "\n",
       "The function that is minimized is:\n",
       "\n",
       ".. math::\n",
       "\n",
       "    0.5*RSS/n + alpha*((1-L1\\_wt)*|params|_2^2/2 + L1\\_wt*|params|_1)\n",
       "\n",
       "where RSS is the usual regression sum of squares, n is the\n",
       "sample size, and :math:`|*|_1` and :math:`|*|_2` are the L1 and L2\n",
       "norms.\n",
       "\n",
       "For WLS and GLS, the RSS is calculated using the whitened endog and\n",
       "exog data.\n",
       "\n",
       "Post-estimation results are based on the same data used to\n",
       "select variables, hence may be subject to overfitting biases.\n",
       "\n",
       "The elastic_net method uses the following keyword arguments:\n",
       "\n",
       "maxiter : int\n",
       "    Maximum number of iterations\n",
       "cnvrg_tol : float\n",
       "    Convergence threshold for line searches\n",
       "zero_tol : float\n",
       "    Coefficients below this threshold are treated as zero.\n",
       "\n",
       "The square root lasso approach is a variation of the Lasso\n",
       "that is largely self-tuning (the optimal tuning parameter\n",
       "does not depend on the standard deviation of the regression\n",
       "errors).  If the errors are Gaussian, the tuning parameter\n",
       "can be taken to be\n",
       "\n",
       "alpha = 1.1 * np.sqrt(n) * norm.ppf(1 - 0.05 / (2 * p))\n",
       "\n",
       "where n is the sample size and p is the number of predictors.\n",
       "\n",
       "The square root lasso uses the following keyword arguments:\n",
       "\n",
       "zero_tol : float\n",
       "    Coefficients below this threshold are treated as zero.\n",
       "\n",
       "The cvxopt module is required to estimate model using the square root\n",
       "lasso.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [*] Friedman, Hastie, Tibshirani (2008).  Regularization paths for\n",
       "   generalized linear models via coordinate descent.  Journal of\n",
       "   Statistical Software 33(1), 1-22 Feb 2010.\n",
       "\n",
       ".. [*] A Belloni, V Chernozhukov, L Wang (2011).  Square-root Lasso:\n",
       "   pivotal recovery of sparse signals via conic programming.\n",
       "   Biometrika 98(4), 791-806. https://arxiv.org/pdf/1009.5689.pdf\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\chris\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit = sm.OLS(y_train, x_train)\n",
    "fit.fit_regularized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const                0.0\n",
       "pos_net_jobs         0.0\n",
       "emp_estabs           0.0\n",
       "estabs_entry_rate    0.0\n",
       "estabs_exit_rate     0.0\n",
       "pop                  0.0\n",
       "pop_pct_black        0.0\n",
       "pop_pct_hisp         0.0\n",
       "lfpr                 0.0\n",
       "density              0.0\n",
       "lower                0.0\n",
       "similar              0.0\n",
       "2003                 0.0\n",
       "2004                 0.0\n",
       "2005                 0.0\n",
       "2006                 0.0\n",
       "2007                 0.0\n",
       "2008                 0.0\n",
       "2009                 0.0\n",
       "2010                 0.0\n",
       "2011                 0.0\n",
       "2012                 0.0\n",
       "2013                 0.0\n",
       "2014                 0.0\n",
       "2015                 0.0\n",
       "2016                 0.0\n",
       "2017                 0.0\n",
       "2018                 0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 5-fold cross-validation grid search with a random state of 490. \n",
    "Identify the optimally tuned hyperparameters.\n",
    "Use this grid:\n",
    "```\n",
    "param_grid = {'alpha': 10.**np.arange(-5, -1, 1), \n",
    "              'l1_ratio': np.arange(0, 1, 0.1)}\n",
    "```\n",
    "You will get a warning message about convergence.\n",
    "We will discuss it after the workshop.\n",
    "Think about why it occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Question\n",
    "\n",
    "How many models did we just fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Using the tuned hyperparameters, fit your elastic net model with `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the selected features refit\n",
    "\n",
    "- the non-regularized model with standardized features\n",
    "- the non-regularized model with non-standardized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the percent improvement from the null model RMSE to the elastic-net and OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
